{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request                   # Library to fetch webpages\n",
    "from bs4 import BeautifulSoup           # Library to parse data from webpages\n",
    "from time import sleep\n",
    "!pip install selenium\n",
    "from selenium import webdriver \n",
    "import time\n",
    "# consist of all the schema of the data we are extracting\n",
    "from schema import SCHEMA\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login = {\n",
    "    \"username\": \"kanevi2020@sweatmail.com\",\n",
    "    \"password\": \"testpass123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing chrome webdriver\n",
    "def get_browser():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('log-level=3')\n",
    "#     install chromedriver in python directory and add path here\n",
    "    browser = webdriver.Chrome(options=chrome_options,executable_path=r\"\\\\Programs\\\\Python 3.8\\\\chromedriver\")\n",
    "    return browser\n",
    "browser = get_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to glassdoor webiste using login credential \n",
    "def login():\n",
    "    print('loggging to', login[\"username\"])\n",
    "    url = 'https://www.glassdoor.com/profile/login_input.htm'\n",
    "    \n",
    "    browser.get(url)\n",
    "\n",
    "    time.sleep(1)                           # Give Javascript time to render\n",
    "    email_field = browser.find_element_by_id('userEmail')\n",
    "    time.sleep(1)\n",
    "    password_field = browser.find_element_by_id('userPassword')\n",
    "    submit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\n",
    "\n",
    "    email_field.send_keys(login[\"username\"])\n",
    "    password_field.send_keys(login[\"password\"])\n",
    "    submit_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame([], columns=SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the link of the webpage from which we data need to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_url = 'https://www.glassdoor.com/Reviews/Amazon-Reviews-E6036.htm'\n",
    "browser.get(amazon_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts data from the mentioned class\n",
    "def extract_from_page():\n",
    "\n",
    "    def is_featured(review):\n",
    "        try:\n",
    "            review.find_element_by_class_name('featuredFlag')\n",
    "            return True\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            return False\n",
    "        \n",
    "# extracts author inforamtion - employee status, location , Job role\n",
    "    def extract_review(review):\n",
    "        author = review.find_element_by_class_name('authorInfo')\n",
    "        res = {}\n",
    "        # import pdb;pdb.set_trace()\n",
    "        for field in SCHEMA:\n",
    "            res[field] = scrape(field, review, author)\n",
    "\n",
    "        assert set(res.keys()) == set(SCHEMA)\n",
    "        return res\n",
    "\n",
    "    res = pd.DataFrame([], columns=SCHEMA)\n",
    "\n",
    "    reviews = browser.find_elements_by_class_name('empReview')\n",
    "\n",
    "    for review in reviews:\n",
    "        if not is_featured(review):\n",
    "            data = extract_review(review)\n",
    "            res.loc[idx[0]] = data\n",
    "        else:\n",
    "            print('Discarding a featured review')\n",
    "        idx[0] = idx[0] + 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are more than one page for the company\n",
    "def more_pages():\n",
    "    try:\n",
    "        next_ = browser.find_element_by_class_name('pagination__PaginationStyle__next')\n",
    "        next_.find_element_by_tag_name('a')\n",
    "        return True\n",
    "    except selenium.common.exceptions.NoSuchElementException:\n",
    "        return False\n",
    "# extracting information in the next page using pagination tag in the page \n",
    "def go_to_next_page():\n",
    "    next_ = browser.find_element_by_class_name(\n",
    "        'pagination__PaginationStyle__next').find_element_by_tag_name('a')\n",
    "    browser.get(next_.get_attribute('href'))\n",
    "    time.sleep(1)\n",
    "    page[0] = page[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(field, review, author):\n",
    "# extracts date and time \n",
    "    def scrape_date(review):\n",
    "        return review.find_element_by_tag_name(\n",
    "            'time').get_attribute('datetime')\n",
    "\n",
    "    def scrape_emp_title(review):\n",
    "        if 'Anonymous Employee' not in review.text:\n",
    "            try:\n",
    "                res = author.find_element_by_class_name(\n",
    "                    'authorJobTitle').text.split('-')[1]\n",
    "            except Exception:\n",
    "                res = np.nan\n",
    "        else:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_location(review):\n",
    "        if 'in' in review.text:\n",
    "            try:\n",
    "                res = author.find_element_by_class_name(\n",
    "                    'authorLocation').text\n",
    "            except Exception:\n",
    "                res = np.nan\n",
    "        else:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_status(review):\n",
    "        try:\n",
    "            res = author.text.split('-')[0]\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_rev_title(review):\n",
    "        return review.find_element_by_class_name('summary').text.strip('\"')\n",
    "\n",
    "    def expand_show_more(section):\n",
    "        try:\n",
    "            more_link = section.find_element_by_class_name('v2__EIReviewDetailsV2__continueReading')\n",
    "            more_link.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def scrape_pros(review):\n",
    "        try:\n",
    "            res = review.find_elements_by_class_name(\n",
    "                'v2__EIReviewDetailsV2__fullWidth')[0].find_elements_by_tag_name('p')[1].text\n",
    "            #res = res.strip()\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_cons(review):\n",
    "        try:\n",
    "            res = review.find_elements_by_class_name(\n",
    "                'v2__EIReviewDetailsV2__fullWidth')[1].find_elements_by_tag_name('p')[1].text\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "\n",
    "    def scrape_overall_rating(review):\n",
    "        try:\n",
    "            ratings = review.find_element_by_class_name('gdStars')\n",
    "            overall = ratings.find_element_by_class_name(\n",
    "                'rating').find_element_by_class_name('value-title')\n",
    "            res = overall.get_attribute('title')\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    funcs = [\n",
    "        scrape_date,\n",
    "        scrape_emp_title,\n",
    "        scrape_location,\n",
    "        scrape_status,\n",
    "        scrape_rev_title,\n",
    "        scrape_pros,\n",
    "        scrape_cons,\n",
    "        scrape_overall_rating,\n",
    "    ]\n",
    "    \n",
    "    fdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\n",
    "\n",
    "    return fdict[field](review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = [1]\n",
    "idx = [0]\n",
    "res = pd.DataFrame()\n",
    "reviews_df = extract_from_page()\n",
    "res = res.append(reviews_df)\n",
    "date_limit_reached = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the number of review required as length of results\n",
    "while more_pages() and\\\n",
    "        len(res) < 20:\n",
    "    go_to_next_page()\n",
    "    reviews_df = extract_from_page()\n",
    "    res = res.append(reviews_df)\n",
    "    time.sleep(1)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('Amazon_Review_Output1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
